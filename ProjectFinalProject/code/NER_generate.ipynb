{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Import Packages##############################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords # import packge\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('news20190406_285.csv')\n",
    "newsdocument = list(news['NewsContent']) # a list of news with len = 285\n",
    "\n",
    "def docu_preprocess(News): # newsdocument[i], one single news\n",
    "    '''input: a news\n",
    "       output: a list of tuples containing the individual words in the sentence and their associated part-of-speech'''\n",
    "    #newsdocument = str(';'.join(News))                     # join all news into a string\n",
    "    newsToken = nltk.word_tokenize(News)                    # Tokenize the news\n",
    "    pos_tag = nltk.pos_tag(newsToken)                       #return pos_tag of the news\n",
    "    return pos_tag\n",
    "\n",
    "newsToken_POS = [docu_preprocess(i) for i in newsdocument] # list of news token using docu_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### ______NLTK _________#########################\n",
    "\n",
    "\n",
    "# Use NLTK Package to generate NER and POS_tag in each news\n",
    "iob_tagList = []\n",
    "ne_treeList = []\n",
    "for i in range(len(newsToken_POS)):\n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'                            # chunk pattern, can be modified latter\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    cs = cp.parse(newsToken_POS[i])\n",
    "    iob_tagged = tree2conlltags(cs) # return a list of tuples includes text,POS_tags, and classification of NER\n",
    "    iob_tagList.append(iob_tagged)\n",
    "    ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(newsdocument[i]))) # use function nltk.ne_clunk to reconginize named entity using a classifier\n",
    "    ne_treeList.append(ne_tree)\n",
    "\n",
    "print(iob_tagList) # list of tuples, contain text,POS_tags, and classification of NER\n",
    "print(ne_treeList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################_______SpaCy_________#########################\n",
    "# use SpaCy Package to generate NER and POS_tag in each news\n",
    "nlp = en_core_web_sm.load()\n",
    "for i in range(len(newsdocument)):\n",
    "    doc = nlp(str(newsdocument[i]))\n",
    "    SpaCy_POS =[(x.orth_,x.pos_,x.tag_,x.dep_) for x in doc] # retuen Verbatim text content, POS, Tag,\n",
    "    SpaCy_NER = [(x.text, x.label_) for x in doc.ents] # return text, NER\n",
    "\n",
    "print(SpaCy_POS)\n",
    "print(SpaCy_NER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
