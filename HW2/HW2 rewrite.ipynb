{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import nltk\n",
    "import gensim \n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这就是个定义拿data 的function\n",
    "categories = ['talk.politics.guns','rec.sport.baseball'] # We focus on 2 news categories\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              categories=categories,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.sport.baseball', 'talk.politics.guns']\n",
      "Sample document: For those who didn't figure it out, the below message was a reply to another\n",
      "in sci.crypt, for which the poster put t.p.g. in the Followup-To line. I\n",
      "didn't notice that. Apologies to those who were confused.\n",
      "\n",
      "The substance makes little sense unless one reads the prior messages.\n",
      "\n",
      "However, I don't wish to enter into this discussion here, as it will be yet\n",
      "another rehearsal of a long-tired set of arguments. Suffice it to say that I\n",
      "disagree both with the interpretation of \"well-regulated\" in the Second\n",
      "Amendment offered by gun lovers, and what I think to be their distortion of\n",
      "the same phrase in the associated Federalist papers. My Webster and my\n",
      "reading of the language convinces me that the word meant both under control,\n",
      "and disciplined, and not 'of good marksmanship'. I think the latter a\n",
      "special interest pleading. No one has yet shown a contemporateous reference\n",
      "in which \"well regulated\" unambiguously meant 'of good marksmanship', and\n",
      "not under control/disciplined, etc.\n",
      "\n",
      "Thus I continue to believe the Second Amendment is a militia clause and not\n",
      "an 'arming everyone' clause. Others are welcome to disagree (as I know many\n",
      "do) and little would be served by rehashing this topic in this particular\n",
      "forum.\n",
      "\n",
      "To avoid flames, or unproductive rehashings, I note that I've come in here\n",
      "to post this one message, just to clarify the one below. I'm now outta here\n",
      "again though I'm available via e-mail.\n",
      "\n",
      "David\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Class label: 1\n",
      "Actual class label: talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# get text data and their labels\n",
    "dataset = get_data()\n",
    "print(dataset.target_names)\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "\n",
    "print('Sample document:', corpus[10])\n",
    "print('Class label:',labels[10])\n",
    "print('Actual class label:', dataset.target_names[labels[10]])\n",
    "\n",
    "# split training dataset and testing dataset\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000152</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>008</th>\n",
       "      <th>0094</th>\n",
       "      <th>...</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zimring</th>\n",
       "      <th>zip</th>\n",
       "      <th>zog</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zot</th>\n",
       "      <th>zupcic</th>\n",
       "      <th>zzzzzzt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000152  000th  001  002  003  004  008  0094   ...     zilch  \\\n",
       "0   0    0       0      0    0    0    0    0    0     0   ...         0   \n",
       "1   0    0       0      0    0    0    0    0    0     0   ...         0   \n",
       "2   0    0       0      0    0    0    0    0    0     0   ...         0   \n",
       "3   0    0       0      0    0    0    0    0    0     0   ...         0   \n",
       "4   0    0       0      0    0    0    0    0    0     0   ...         0   \n",
       "\n",
       "   zimring  zip  zog  zone  zones  zoning  zot  zupcic  zzzzzzt  \n",
       "0        0    0    0     0      0       0    0       0        0  \n",
       "1        0    0    0     0      0       0    0       0        0  \n",
       "2        0    0    0     0      0       0    0       0        0  \n",
       "3        0    0    0     0      0       0    0       0        0  \n",
       "4        0    0    0     0      0       0    0       0        0  \n",
       "\n",
       "[5 rows x 15773 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bow features\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "\n",
    "# build bag of words features' vectorizer and get features\n",
    "bow_vectorizer=CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "\n",
    "\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus) \n",
    "\n",
    "\n",
    "bow_train_features.A # Array 形式 \n",
    "dftr_count = pd.DataFrame(data=bow_train_features.A, columns=bow_vectorizer.get_feature_names())\n",
    "dftr_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000152</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>008</th>\n",
       "      <th>0094</th>\n",
       "      <th>...</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zimring</th>\n",
       "      <th>zip</th>\n",
       "      <th>zog</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zot</th>\n",
       "      <th>zupcic</th>\n",
       "      <th>zzzzzzt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000152  000th  001  002  003  004  008  0094   ...     zilch  \\\n",
       "0  0.0  0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0   ...       0.0   \n",
       "1  0.0  0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0   ...       0.0   \n",
       "2  0.0  0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0   ...       0.0   \n",
       "3  0.0  0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0   ...       0.0   \n",
       "4  0.0  0.0     0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0   ...       0.0   \n",
       "\n",
       "   zimring  zip  zog  zone  zones  zoning  zot  zupcic  zzzzzzt  \n",
       "0      0.0  0.0  0.0   0.0    0.0     0.0  0.0     0.0      0.0  \n",
       "1      0.0  0.0  0.0   0.0    0.0     0.0  0.0     0.0      0.0  \n",
       "2      0.0  0.0  0.0   0.0    0.0     0.0  0.0     0.0      0.0  \n",
       "3      0.0  0.0  0.0   0.0    0.0     0.0  0.0     0.0      0.0  \n",
       "4      0.0  0.0  0.0   0.0    0.0     0.0  0.0     0.0      0.0  \n",
       "\n",
       "[5 rows x 15773 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus) \n",
    "dftr_tfidf = pd.DataFrame(data=tfidf_train_features.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "dftr_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这里讲的是word to vect了 \n",
    "#这和上面的chart 内容一点都没关系\n",
    "\n",
    "# tokenize documents for word2vec\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in test_corpus]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['As',\n",
       "  'far',\n",
       "  'as',\n",
       "  'I',\n",
       "  'know',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'FAQ',\n",
       "  'for',\n",
       "  'tpg',\n",
       "  '.',\n",
       "  'Somebody',\n",
       "  'was',\n",
       "  'working',\n",
       "  'on',\n",
       "  'one',\n",
       "  ',',\n",
       "  'but',\n",
       "  'I',\n",
       "  'think',\n",
       "  'it',\n",
       "  '``',\n",
       "  'died',\n",
       "  'in',\n",
       "  'committee',\n",
       "  '.',\n",
       "  \"''\"],\n",
       " ['I',\n",
       "  'like',\n",
       "  'Alomar',\n",
       "  '.',\n",
       "  'But',\n",
       "  'I',\n",
       "  \"'d\",\n",
       "  'like',\n",
       "  'to',\n",
       "  'differ',\n",
       "  'with',\n",
       "  'your',\n",
       "  'opinion',\n",
       "  'about',\n",
       "  '``',\n",
       "  'a',\n",
       "  'city',\n",
       "  'which',\n",
       "  'is',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'pour',\n",
       "  'in',\n",
       "  'the',\n",
       "  'votes',\n",
       "  '...',\n",
       "  \"''\",\n",
       "  '.',\n",
       "  'I',\n",
       "  'attended',\n",
       "  'many',\n",
       "  'games',\n",
       "  'last',\n",
       "  'year',\n",
       "  'during',\n",
       "  'the',\n",
       "  'balloting',\n",
       "  '.',\n",
       "  'I',\n",
       "  'know',\n",
       "  'that',\n",
       "  'a',\n",
       "  'great',\n",
       "  'number',\n",
       "  'of',\n",
       "  'the',\n",
       "  'attendees',\n",
       "  'DID',\n",
       "  'NOT',\n",
       "  'fill',\n",
       "  'out',\n",
       "  'their',\n",
       "  'ballots',\n",
       "  ',',\n",
       "  'but',\n",
       "  'left',\n",
       "  'them',\n",
       "  ',',\n",
       "  'beer',\n",
       "  'soaked',\n",
       "  'and',\n",
       "  'torn',\n",
       "  'on',\n",
       "  'the',\n",
       "  'floor',\n",
       "  'of',\n",
       "  'the',\n",
       "  'stands',\n",
       "  '.',\n",
       "  'Toronto',\n",
       "  'gets',\n",
       "  'no',\n",
       "  'more',\n",
       "  'and',\n",
       "  'no',\n",
       "  'less',\n",
       "  'votes',\n",
       "  'than',\n",
       "  'any',\n",
       "  'other',\n",
       "  'city',\n",
       "  'for',\n",
       "  'the',\n",
       "  'All',\n",
       "  'Star',\n",
       "  'game',\n",
       "  '.',\n",
       "  'Unfortunately',\n",
       "  ',',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'one',\n",
       "  'time',\n",
       "  'thing',\n",
       "  '.',\n",
       "  'I',\n",
       "  \"'ve\",\n",
       "  'attended',\n",
       "  'games',\n",
       "  'during',\n",
       "  'the',\n",
       "  'last',\n",
       "  'four',\n",
       "  'seasons',\n",
       "  ',',\n",
       "  'and',\n",
       "  'it',\n",
       "  'has',\n",
       "  'happened',\n",
       "  'every',\n",
       "  'time',\n",
       "  '.',\n",
       "  'The',\n",
       "  'apathetic',\n",
       "  'attitude',\n",
       "  'to',\n",
       "  'All',\n",
       "  'Star',\n",
       "  'ballots',\n",
       "  'really',\n",
       "  'offends',\n",
       "  'me',\n",
       "  '.'],\n",
       " [':',\n",
       "  'In',\n",
       "  'article',\n",
       "  '<',\n",
       "  '0096B294.AAD9C1E0',\n",
       "  '@',\n",
       "  'uinpla.npl.uiuc.edu',\n",
       "  '>',\n",
       "  'reimer',\n",
       "  '@',\n",
       "  'uinpla.npl.uiuc.edu',\n",
       "  ':',\n",
       "  '>',\n",
       "  'In',\n",
       "  'article',\n",
       "  '<',\n",
       "  '1qkftjINNoij',\n",
       "  '@',\n",
       "  'cronkite.cisco.com',\n",
       "  '>',\n",
       "  ',',\n",
       "  'pitargue',\n",
       "  '@',\n",
       "  'cisco.com',\n",
       "  '(',\n",
       "  'Marciano',\n",
       "  ':',\n",
       "  '>',\n",
       "  ':',\n",
       "  '>',\n",
       "  '[',\n",
       "  'stuff',\n",
       "  'deleted',\n",
       "  'about',\n",
       "  'causes',\n",
       "  'of',\n",
       "  'people',\n",
       "  'in',\n",
       "  'ER',\n",
       "  ']',\n",
       "  ':',\n",
       "  '>',\n",
       "  ':',\n",
       "  '>',\n",
       "  '>',\n",
       "  'due',\n",
       "  'to',\n",
       "  'automobile',\n",
       "  'accidents',\n",
       "  'and',\n",
       "  'automobile',\n",
       "  'crimes',\n",
       "  '.',\n",
       "  'maybe',\n",
       "  'we',\n",
       "  'should',\n",
       "  'outlaw',\n",
       "  ':',\n",
       "  '>',\n",
       "  '>',\n",
       "  'cars',\n",
       "  '.',\n",
       "  ':',\n",
       "  '>',\n",
       "  'There',\n",
       "  'are',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'automobile',\n",
       "  'accidents',\n",
       "  ',',\n",
       "  'but',\n",
       "  'atleast',\n",
       "  'there',\n",
       "  'is',\n",
       "  'some',\n",
       "  ':',\n",
       "  '>',\n",
       "  'regulation',\n",
       "  'to',\n",
       "  'try',\n",
       "  'to',\n",
       "  'combat',\n",
       "  'this',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Such',\n",
       "  'as',\n",
       "  '?',\n",
       "  'Drunk',\n",
       "  'drivers',\n",
       "  'get',\n",
       "  'back',\n",
       "  'on',\n",
       "  'the',\n",
       "  'road',\n",
       "  'in',\n",
       "  'no',\n",
       "  'time',\n",
       "  ',',\n",
       "  'to',\n",
       "  'kill',\n",
       "  'again',\n",
       "  '.',\n",
       "  'Seems',\n",
       "  ':',\n",
       "  'the',\n",
       "  'driver',\n",
       "  \"'s\",\n",
       "  'license',\n",
       "  'process',\n",
       "  'does',\n",
       "  'not',\n",
       "  'work',\n",
       "  'for',\n",
       "  'this',\n",
       "  '.',\n",
       "  ':',\n",
       "  'I',\n",
       "  'can',\n",
       "  'testify',\n",
       "  'to',\n",
       "  'this',\n",
       "  '.',\n",
       "  'My',\n",
       "  'cousin',\n",
       "  'spent',\n",
       "  'a',\n",
       "  'few',\n",
       "  'weeks',\n",
       "  'in',\n",
       "  'the',\n",
       "  'hospital',\n",
       "  ',',\n",
       "  'and',\n",
       "  'his',\n",
       "  'friend',\n",
       "  'was',\n",
       "  'killed',\n",
       "  ',',\n",
       "  'because',\n",
       "  'of',\n",
       "  'a',\n",
       "  'drunk',\n",
       "  'driver',\n",
       "  '.',\n",
       "  'The',\n",
       "  'son-of-a-b****',\n",
       "  'is',\n",
       "  'back',\n",
       "  'on',\n",
       "  'the',\n",
       "  'streets',\n",
       "  '...',\n",
       "  'Officers',\n",
       "  'from',\n",
       "  'the',\n",
       "  'scene',\n",
       "  'are',\n",
       "  'still',\n",
       "  'p***ed',\n",
       "  'about',\n",
       "  'that',\n",
       "  'one',\n",
       "  '.',\n",
       "  ':',\n",
       "  '>',\n",
       "  'to',\n",
       "  'take',\n",
       "  'a',\n",
       "  'drivers',\n",
       "  'safety',\n",
       "  'class',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Because',\n",
       "  'you',\n",
       "  'wanted',\n",
       "  'one',\n",
       "  'while',\n",
       "  'you',\n",
       "  'were',\n",
       "  'underage',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  '>',\n",
       "  'I',\n",
       "  'HAVE',\n",
       "  'to',\n",
       "  'be',\n",
       "  'licensed',\n",
       "  'to',\n",
       "  'drive',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Only',\n",
       "  'on',\n",
       "  'public',\n",
       "  'roads',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  '>',\n",
       "  'My',\n",
       "  'car',\n",
       "  ':',\n",
       "  '>',\n",
       "  'MUST',\n",
       "  'be',\n",
       "  'registered',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Only',\n",
       "  'if',\n",
       "  'it',\n",
       "  'is',\n",
       "  'to',\n",
       "  'be',\n",
       "  'driven',\n",
       "  'on',\n",
       "  'public',\n",
       "  'roads',\n",
       "  ',',\n",
       "  'other',\n",
       "  'than',\n",
       "  'between',\n",
       "  'segments',\n",
       "  'of',\n",
       "  'my',\n",
       "  ':',\n",
       "  'property',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  '>',\n",
       "  'I',\n",
       "  'MUST',\n",
       "  '(',\n",
       "  'at',\n",
       "  'least',\n",
       "  'where',\n",
       "  'I',\n",
       "  'live',\n",
       "  ')',\n",
       "  'have',\n",
       "  'liability',\n",
       "  ':',\n",
       "  '>',\n",
       "  'insurance',\n",
       "  'on',\n",
       "  'both',\n",
       "  'myself',\n",
       "  'driving',\n",
       "  'and',\n",
       "  'my',\n",
       "  'car',\n",
       "  '(',\n",
       "  'if',\n",
       "  'someone',\n",
       "  'else',\n",
       "  'had',\n",
       "  'an',\n",
       "  ':',\n",
       "  '>',\n",
       "  'accident',\n",
       "  'with',\n",
       "  'it',\n",
       "  ')',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Only',\n",
       "  'on',\n",
       "  'public',\n",
       "  'roads',\n",
       "  '.',\n",
       "  'And',\n",
       "  'this',\n",
       "  'obviously',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'always',\n",
       "  'work',\n",
       "  ',',\n",
       "  'else',\n",
       "  'why',\n",
       "  'would',\n",
       "  'they',\n",
       "  'offer',\n",
       "  'uninsured',\n",
       "  'motorist',\n",
       "  'coverage',\n",
       "  '?',\n",
       "  ':',\n",
       "  ':',\n",
       "  '>',\n",
       "  'Hmm',\n",
       "  ',',\n",
       "  'would',\n",
       "  \"n't\",\n",
       "  'manditory',\n",
       "  'saftey',\n",
       "  'classes',\n",
       "  ',',\n",
       "  'registration',\n",
       "  ':',\n",
       "  '>',\n",
       "  'of',\n",
       "  'both',\n",
       "  'the',\n",
       "  'owner',\n",
       "  'and',\n",
       "  'gun',\n",
       "  ',',\n",
       "  'and',\n",
       "  'manditory',\n",
       "  'liability',\n",
       "  'insurance',\n",
       "  'be',\n",
       "  'nice',\n",
       "  'for',\n",
       "  ':',\n",
       "  '>',\n",
       "  'gun',\n",
       "  'owners',\n",
       "  '.',\n",
       "  'I',\n",
       "  'object',\n",
       "  'to',\n",
       "  'mandatory',\n",
       "  'registration',\n",
       "  'because',\n",
       "  'I',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'trust',\n",
       "  'my',\n",
       "  'government',\n",
       "  'not',\n",
       "  'to',\n",
       "  'use',\n",
       "  'any',\n",
       "  'information',\n",
       "  'I',\n",
       "  'give',\n",
       "  'them',\n",
       "  'for',\n",
       "  'their',\n",
       "  'own',\n",
       "  'purposes',\n",
       "  '.',\n",
       "  'I',\n",
       "  'am',\n",
       "  'licensed',\n",
       "  'to',\n",
       "  'carry',\n",
       "  'a',\n",
       "  'concealed',\n",
       "  'pistol',\n",
       "  'in',\n",
       "  'my',\n",
       "  'home',\n",
       "  'state',\n",
       "  ',',\n",
       "  'but',\n",
       "  'they',\n",
       "  'never',\n",
       "  'asked',\n",
       "  'whether',\n",
       "  'I',\n",
       "  'actually',\n",
       "  'owned',\n",
       "  'a',\n",
       "  'firearm',\n",
       "  '.',\n",
       "  'A',\n",
       "  'safety',\n",
       "  'class',\n",
       "  'before',\n",
       "  'issuing',\n",
       "  'a',\n",
       "  'permit',\n",
       "  'to',\n",
       "  'carry',\n",
       "  'is',\n",
       "  'reasonably',\n",
       "  ',',\n",
       "  'provided',\n",
       "  'such',\n",
       "  'classes',\n",
       "  'are',\n",
       "  'regularly',\n",
       "  'available',\n",
       "  'to',\n",
       "  'the',\n",
       "  'public',\n",
       "  '.',\n",
       "  'Of',\n",
       "  'course',\n",
       "  ',',\n",
       "  'most',\n",
       "  'places',\n",
       "  'would',\n",
       "  'consider',\n",
       "  'my',\n",
       "  'time',\n",
       "  'in',\n",
       "  'the',\n",
       "  'reserves',\n",
       "  'and',\n",
       "  'on',\n",
       "  'a',\n",
       "  'competition',\n",
       "  'rifle',\n",
       "  'team',\n",
       "  'to',\n",
       "  'count',\n",
       "  '.',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Perhaps',\n",
       "  ',',\n",
       "  'if',\n",
       "  'it',\n",
       "  'gave',\n",
       "  'them',\n",
       "  'permission',\n",
       "  'to',\n",
       "  'shoot',\n",
       "  'in',\n",
       "  'public',\n",
       "  'roads',\n",
       "  'and',\n",
       "  'parks',\n",
       "  '.',\n",
       "  ':',\n",
       "  '-',\n",
       "  ')',\n",
       "  'Hey',\n",
       "  ',',\n",
       "  'now',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'an',\n",
       "  'idea',\n",
       "  ':',\n",
       "  ')',\n",
       "  ':',\n",
       "  ':',\n",
       "  '>',\n",
       "  'Paul',\n",
       "  'Reimer',\n",
       "  ':',\n",
       "  ':',\n",
       "  'Jim',\n",
       "  'Now',\n",
       "  ',',\n",
       "  'unless',\n",
       "  'you',\n",
       "  'have',\n",
       "  'an',\n",
       "  'agenda',\n",
       "  'against',\n",
       "  'private',\n",
       "  'ownership',\n",
       "  'of',\n",
       "  'firearms',\n",
       "  ',',\n",
       "  'why',\n",
       "  'would',\n",
       "  'you',\n",
       "  'want',\n",
       "  'to',\n",
       "  'harass',\n",
       "  'the',\n",
       "  'person',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'legally',\n",
       "  'defend',\n",
       "  'themselves',\n",
       "  'or',\n",
       "  'exercise',\n",
       "  'their',\n",
       "  'rights',\n",
       "  '?',\n",
       "  '(',\n",
       "  'I',\n",
       "  'know',\n",
       "  ',',\n",
       "  'defending',\n",
       "  'oneself/family/whoever',\n",
       "  'IS',\n",
       "  'a',\n",
       "  'right',\n",
       "  '...',\n",
       "  'at',\n",
       "  'least',\n",
       "  'as',\n",
       "  'far',\n",
       "  'as',\n",
       "  'my',\n",
       "  '9mm',\n",
       "  'and',\n",
       "  'I',\n",
       "  'are',\n",
       "  'concerned',\n",
       "  '...',\n",
       "  ')',\n",
       "  '(',\n",
       "  'Also',\n",
       "  'as',\n",
       "  'far',\n",
       "  'as',\n",
       "  'the',\n",
       "  'State',\n",
       "  'of',\n",
       "  'Alabama',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'be',\n",
       "  'concerned',\n",
       "  ')',\n",
       "  'Why',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'you',\n",
       "  'push',\n",
       "  'for',\n",
       "  'stricter',\n",
       "  'prosecution',\n",
       "  'of',\n",
       "  'those',\n",
       "  'who',\n",
       "  'use',\n",
       "  'firearms',\n",
       "  'in',\n",
       "  'the',\n",
       "  'commission',\n",
       "  'of',\n",
       "  'a',\n",
       "  'crime',\n",
       "  '?',\n",
       "  'I',\n",
       "  \"'ve\",\n",
       "  'already',\n",
       "  'pointed',\n",
       "  'out',\n",
       "  'how',\n",
       "  'we',\n",
       "  'are',\n",
       "  \"n't\",\n",
       "  'nailing',\n",
       "  'DUI',\n",
       "  \"'s\",\n",
       "  'hard',\n",
       "  'enough',\n",
       "  '...',\n",
       "  'Comparing',\n",
       "  'the',\n",
       "  'US',\n",
       "  'with',\n",
       "  'other',\n",
       "  'countries',\n",
       "  'seldom',\n",
       "  'works',\n",
       "  ',',\n",
       "  'but',\n",
       "  'the',\n",
       "  'European',\n",
       "  'attitude',\n",
       "  'towards',\n",
       "  'alchohol',\n",
       "  'and',\n",
       "  'DUI',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'work..',\n",
       "  'Their',\n",
       "  'attitude',\n",
       "  'towards',\n",
       "  'weapons',\n",
       "  'is',\n",
       "  \"n't\",\n",
       "  'really',\n",
       "  'a',\n",
       "  'valid',\n",
       "  'comparison',\n",
       "  'because',\n",
       "  \"they've\",\n",
       "  'historically',\n",
       "  'done',\n",
       "  'their',\n",
       "  'best',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'the',\n",
       "  'populace',\n",
       "  'disarmed',\n",
       "  'and',\n",
       "  'submissive',\n",
       "  ',',\n",
       "  'while',\n",
       "  'our',\n",
       "  'country',\n",
       "  'was',\n",
       "  'founded',\n",
       "  'by',\n",
       "  'a',\n",
       "  'bunch',\n",
       "  'of',\n",
       "  'rugged',\n",
       "  'individualists',\n",
       "  'who',\n",
       "  'told',\n",
       "  'the',\n",
       "  'European',\n",
       "  'monarchies',\n",
       "  '(',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'part',\n",
       "  ')',\n",
       "  'to',\n",
       "  'take',\n",
       "  'a',\n",
       "  'flying',\n",
       "  'leap',\n",
       "  '(',\n",
       "  'used',\n",
       "  'more',\n",
       "  'polite',\n",
       "  'language',\n",
       "  'though',\n",
       "  ')',\n",
       "  '.',\n",
       "  'We',\n",
       "  'even',\n",
       "  'weaseled',\n",
       "  'out',\n",
       "  'of',\n",
       "  'our',\n",
       "  'first',\n",
       "  'international',\n",
       "  'treaty',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'convinced',\n",
       "  'the',\n",
       "  'French',\n",
       "  'that',\n",
       "  'it',\n",
       "  'was',\n",
       "  'in',\n",
       "  'their',\n",
       "  'best',\n",
       "  'interests',\n",
       "  'not',\n",
       "  'to',\n",
       "  'complain..',\n",
       "  'But',\n",
       "  'first',\n",
       "  'we',\n",
       "  'had',\n",
       "  'to',\n",
       "  'overcome',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'the',\n",
       "  'Brits',\n",
       "  'were',\n",
       "  'doing',\n",
       "  'their',\n",
       "  'best',\n",
       "  'to',\n",
       "  'restrict',\n",
       "  'us',\n",
       "  'to',\n",
       "  'squirrel',\n",
       "  'guns',\n",
       "  'and',\n",
       "  'such',\n",
       "  ',',\n",
       "  'so',\n",
       "  'we',\n",
       "  \"'d\",\n",
       "  'be',\n",
       "  'properly',\n",
       "  'submissive',\n",
       "  'while',\n",
       "  'they',\n",
       "  'forced',\n",
       "  'us',\n",
       "  'to',\n",
       "  'pay',\n",
       "  'for',\n",
       "  'their',\n",
       "  'wars',\n",
       "  '.',\n",
       "  'Of',\n",
       "  'course',\n",
       "  ',',\n",
       "  'most',\n",
       "  'American',\n",
       "  'history',\n",
       "  'classes',\n",
       "  'these',\n",
       "  'days',\n",
       "  'tend',\n",
       "  'to',\n",
       "  'gloss',\n",
       "  'over',\n",
       "  'facts',\n",
       "  'that',\n",
       "  'do',\n",
       "  'not',\n",
       "  'fit',\n",
       "  'the',\n",
       "  'image',\n",
       "  'they',\n",
       "  'wish',\n",
       "  'to',\n",
       "  'convey',\n",
       "  '...',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'glad',\n",
       "  'my',\n",
       "  'Amer',\n",
       "  '.',\n",
       "  'Hist',\n",
       "  '.',\n",
       "  'teacher',\n",
       "  'was',\n",
       "  'a',\n",
       "  'Libertarian',\n",
       "  'who',\n",
       "  'had',\n",
       "  'us',\n",
       "  'review',\n",
       "  'a',\n",
       "  'good',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Federalist',\n",
       "  'Papers',\n",
       "  ',',\n",
       "  'and',\n",
       "  'debate',\n",
       "  'their',\n",
       "  'origins',\n",
       "  'and',\n",
       "  'meanings',\n",
       "  '.',\n",
       "  'enough',\n",
       "  'rambling',\n",
       "  ',',\n",
       "  'James'],\n",
       " ['Is',\n",
       "  'there',\n",
       "  'a',\n",
       "  'Chicago',\n",
       "  'Cubs',\n",
       "  'mailing',\n",
       "  'list',\n",
       "  '?',\n",
       "  '?',\n",
       "  'If',\n",
       "  'so',\n",
       "  ',',\n",
       "  'I',\n",
       "  \"'d\",\n",
       "  'like',\n",
       "  'to',\n",
       "  'join',\n",
       "  '.',\n",
       "  'Any',\n",
       "  'help',\n",
       "  'appreciated',\n",
       "  '...',\n",
       "  '.'],\n",
       " []]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec就是要一个一个token的啊，同时还保持了原有文本，一个文本一个list\n",
    "tokenized_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec model                   \n",
    "wv_model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=200,                          #set the size or dimension for the word vectors \n",
    "                               window=60,                        #specify the length of the window of words taken as context\n",
    "                               min_count=10)                   #ignores all words with total frequency lower than                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.1535941 , -1.3129113 ,  0.23458871, -0.33068755,  0.43720597,\n",
       "       -0.13894087, -0.8571105 , -0.00467367,  0.18444167, -0.6611506 ,\n",
       "       -0.1670258 , -0.518119  ,  0.46856806, -0.91829175, -0.37597236,\n",
       "        0.21820845, -0.28196785,  0.00758011, -0.36847767,  0.0980043 ,\n",
       "       -0.38790295,  0.18841547, -0.01788563,  0.44226685, -0.24593763,\n",
       "       -0.7527973 , -0.26011997,  0.01758193,  0.56317204, -0.31191692,\n",
       "        0.10937613, -0.17004125,  0.60209215, -0.24984527, -0.39378774,\n",
       "        0.2490019 ,  0.41330323, -0.35673538, -0.06816277,  0.1013827 ,\n",
       "       -0.18191503,  0.10383177,  0.02907011, -0.43524066,  0.44067192,\n",
       "       -0.02053437,  0.4044874 ,  1.0910525 , -0.6536461 , -0.20244399,\n",
       "        0.31993043, -0.06161489,  0.8232507 , -0.00562888, -0.40769097,\n",
       "        0.27065396, -0.22417703, -0.31463748, -0.14538246, -0.12616844,\n",
       "       -0.01433324, -0.10571785, -0.1433176 ,  0.15190828, -0.24930778,\n",
       "       -0.06964333, -0.21610467,  0.06465088, -0.3740505 ,  0.3530064 ,\n",
       "       -0.35001662, -0.09261911, -0.10755189, -0.7948285 , -0.46535447,\n",
       "        0.41998154, -0.01858038,  0.09469368,  0.48510158,  0.41298392,\n",
       "        0.2262804 , -0.3391418 , -0.13586368,  0.9374511 ,  0.21335271,\n",
       "       -0.04941018, -0.03114457, -0.02868977, -0.11988212,  0.3966801 ,\n",
       "       -0.1873766 , -0.12539674, -0.04328665,  0.16097458, -0.37681186,\n",
       "       -0.00536537, -0.35710534,  0.19025859,  0.07076281,  0.8306608 ,\n",
       "        0.6985899 , -0.24941912, -0.5805635 ,  0.8568653 ,  0.04110567,\n",
       "        0.18344823,  0.28300667,  0.39407086,  0.12318686, -0.13679726,\n",
       "       -0.2055481 , -0.23631757,  0.23503931, -0.16283016,  0.12662975,\n",
       "        0.38558552,  0.0985916 ,  0.14090674,  0.310666  , -0.2206207 ,\n",
       "        0.5365811 , -0.19483706, -0.7729535 ,  0.17444703,  0.24687685,\n",
       "        0.1247972 , -0.30176544,  0.82375914, -0.08389003,  0.40460074,\n",
       "        0.24670148, -0.5289152 , -0.6451382 , -0.10389243, -0.00321916,\n",
       "       -0.13356416, -0.14922006, -0.4270003 ,  0.63520926,  0.2686725 ,\n",
       "        0.245565  , -0.26723918, -0.1689075 , -0.08422874,  0.4260889 ,\n",
       "       -0.4278722 ,  0.00811273, -0.00178303,  0.2917574 ,  0.28541762,\n",
       "        0.50037557,  0.10857299,  0.6987542 ,  0.20933776,  0.41411927,\n",
       "        0.05128369,  0.11485078, -0.4846553 , -0.25981164, -0.20974793,\n",
       "       -0.13199648,  0.00235756,  0.3521022 , -0.38688394,  0.1038087 ,\n",
       "        0.22634298,  0.09247115,  0.2676071 , -0.01702454, -0.20887841,\n",
       "        0.19109859, -0.28811106, -0.24715388,  0.08910567,  0.6199224 ,\n",
       "        0.3068095 ,  0.5474753 ,  0.2765539 ,  0.95989245,  0.06094365,\n",
       "       -0.64258707,  0.7879126 ,  0.65663755, -0.25808406,  0.06839149,\n",
       "        0.4069905 , -0.26499894, -0.6717366 ,  0.0491763 , -0.21174966,\n",
       "        0.13239947,  0.10295342,  0.18364812,  0.25683042, -0.91009325,\n",
       "       -0.1508459 , -0.04621489,  0.22717473, -0.06876098,  0.14176959],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model[list(wv_model.wv.vocab)[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words,\n",
    "                         model, \n",
    "                         vocabulary, \n",
    "                         num_features): \n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")#建一个维度与corpus一致的array\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector \n",
    "   \n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus] #将每一个文本，就是每一个新闻，单独地放在 vectors function里头\n",
    "    return np.array(features)\n",
    "#累计相加求平均\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# averaged word vector features from word2vec\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=wv_model,\n",
    "                                                 num_features=200)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=wv_model,\n",
    "                                                num_features=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1332, 200)\n"
     ]
    }
   ],
   "source": [
    "print(avg_wv_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# define a function to evaluate our classification models based on four metrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print ('Accuracy:', np.round(                                                    \n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that trains the model, performs predictions and evaluates the predictions\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Precision: 0.96\n",
      "Recall: 0.94\n",
      "F1 Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.96\n",
      "Recall: 0.89\n",
      "F1 Score: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.95\n",
      "Recall: 0.88\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.98\n",
      "Recall: 0.88\n",
      "F1 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.95\n",
      "Recall: 0.89\n",
      "F1 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "Precision: 0.95\n",
      "Recall: 0.87\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball -> talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# Observe false positive output\n",
    "class_names = dataset.target_names\n",
    "print (class_names[0], '->', class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  I was in fact going to suggest that Roger take his way of discussion over to r.s.football.pro. There this kind of hormone-only reasoning is the standard. Being he canadian, and hockey what it is, I would have suggested that r.s.h would work too. It is important in a thread that everyone involved use the same body part to produce a post (brain being the organ of choice here).\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "If there is a \"USA Today ftp site\" could someone please post it to the     newsgroup so everyone will stop posting the \"send it to me too\" articles.    I'm sure many people woulds like to know so why not just post it to the net    rather than mailing hundreds of people.      Just a thought.        Thanks,    Ryan Robin.\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "         When I say \"black,\" I mean US-born black people for the purposes of this discussion.  Hispanic players were in baseball before 1947, and one  team in the 50's signed lots of hispanics because they went over better with the local audience than blacks did.     Don't know.  But remember: this is the country that had special racial laws for one group and one group only: blacks.  Our national history  includes huge, long-term, global tensions regarding the black minority;  the hispanic minority, while often discriminated against, has never been the object of national obsession.   Absolutely.  As I said before, I expect that this effect is disappearing. But it certainly did exist, and all out talk of TWG's and all that is  not without some small reason.   Well, there's the list.  Go for it!  I'll cull some more names as I go. I expect you're right, btw.\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "    You must not be old enough to remember the A's in KC!\n"
     ]
    }
   ],
   "source": [
    "# Look at some misclassified documents in detail\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 1:\n",
    "        print ('Actual Label:', class_names[label])\n",
    "        print ('Predicted Label:', class_names[predicted_label])\n",
    "        print ('Document:-')\n",
    "        print (re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Model！\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression # logistics \n",
    "\n",
    "from sklearn.linear_model import Perceptron # perceptron\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB # gaussion \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # \n",
    "\n",
    "from sklearn.svm import LinearSVC #linear\n",
    "\n",
    "from sklearn.svm import SVC #\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier # \n",
    "\n",
    "from sklearn.model_selection import KFold #\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import csv\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These model have been tuned\n",
    "# Classifiers that we are going to use\n",
    "clf1 = GaussianNB()\n",
    "\n",
    "#perceptron\n",
    "clf2 = Perceptron(tol=1e-3, random_state=42,alpha= 0.0001, n_iter= 15)\n",
    "\n",
    "#Tuned Decision Tree Parameters: {'min_samples_leaf': 7, 'max_features': 3, 'criterion': 'entropy', 'max_depth': 8\n",
    "clf3 = RandomForestClassifier(n_estimators='warn', criterion='entropy', max_depth=8, \n",
    "                          min_samples_split=2, min_samples_leaf=7, \n",
    "                          min_weight_fraction_leaf=0.0, max_features=3, \n",
    "                          max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                          min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                          n_jobs=None, random_state=None, verbose=0, warm_start=False, \n",
    "                          class_weight=None)\n",
    "\n",
    "#linear SVM\n",
    "clf4_5 = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=3)\n",
    "#SVM {'C': 10, 'degree': 1, 'gamma': 0.01, 'kernel': 'poly'}\n",
    "clf4 = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=1, gamma=0.01,\n",
    "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
    "  shrinking=True, tol=0.001, verbose=3)\n",
    "\n",
    "#k-nearest neighbor\n",
    "clf5 = KNeighborsClassifier(n_neighbors= 41)\n",
    "#'penalty': 'l2', 'C': 0.05179474679231213\n",
    "\n",
    "#Logistics regresion\n",
    "clf6 = LogisticRegression(solver='liblinear', C=0.4393970560760795,penalty='l2')\n",
    "\n",
    "\n",
    "Clf_list = [clf1,clf2,clf3,clf4,clf5,clf6]\n",
    "#Keras\n",
    "\n",
    "\n",
    "#pytorch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "Precision: 0.92\n",
      "Recall: 0.89\n",
      "F1 Score: 0.91\n",
      "\n",
      "Accuracy: 0.91\n",
      "Precision: 0.94\n",
      "Recall: 0.87\n",
      "F1 Score: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perceptron with bag of words features\n",
    "Per_bow_predictions = train_predict_evaluate_model(classifier=clf2,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "print(\n",
    "\n",
    ")\n",
    "# per with tfidf features\n",
    "per_tfidf_predictions = train_predict_evaluate_model(classifier=clf2,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Accuracy: 0.51\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Random_forest with bag of words features\n",
    "rdf_bow_predictions = train_predict_evaluate_model(classifier=clf3,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# rdf with tfidf features\n",
    "rdf_tfidf_predictions = train_predict_evaluate_model(classifier=clf3,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Accuracy: 0.9\n",
      "Precision: 0.95\n",
      "Recall: 0.83\n",
      "F1 Score: 0.89\n",
      "[LibSVM]Accuracy: 0.84\n",
      "Precision: 0.98\n",
      "Recall: 0.68\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# SVM with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=clf4,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=clf4,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Accuracy: 0.9\n",
      "Precision: 0.96\n",
      "Recall: 0.83\n",
      "F1 Score: 0.89\n",
      "[LibLinear]Accuracy: 0.94\n",
      "Precision: 0.96\n",
      "Recall: 0.91\n",
      "F1 Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Linear svm with bag of words features\n",
    "lsvm_bow_predictions = train_predict_evaluate_model(classifier=clf4_5,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "lsvm_tfidf_predictions = train_predict_evaluate_model(classifier=clf4_5,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "Precision: 0.73\n",
      "Recall: 0.42\n",
      "F1 Score: 0.54\n",
      "Accuracy: 0.51\n",
      "Precision: 1.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "# KNN with bag of words features\n",
    "KNN_bow_predictions = train_predict_evaluate_model(classifier=clf5,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# KNN with tfidf features\n",
    "KNN_tfidf_predictions = train_predict_evaluate_model(classifier=clf5,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.97\n",
      "Recall: 0.86\n",
      "F1 Score: 0.91\n",
      "Accuracy: 0.91\n",
      "Precision: 0.95\n",
      "Recall: 0.86\n",
      "F1 Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Logistics with bag of words features\n",
    "log_bow_predictions = train_predict_evaluate_model(classifier=clf6,\n",
    "                                           train_features=bow_train_features.A,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features.A,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "log_tfidf_predictions = train_predict_evaluate_model(classifier=clf6,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparing\n",
    "clf_list = [mnb,clf2,clf1,clf4,clf6]\n",
    "bow = (bow_train_features.A,train_labels,bow_test_features.A,test_labels)\n",
    "\n",
    "\n",
    "tif = (tfidf_train_features.A,train_labels,tfidf_test_features.A,test_labels)\n",
    "w2v = (avg_wv_train_features,train_labels,avg_wv_test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Majority_vote(data,clfs):\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    vote = np.zeros(X_test.shape[0])\n",
    "    for clf in clfs:\n",
    "        clf.fit(X_train,y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        vote = np.add(vote,predictions)\n",
    "        print(clf)\n",
    "        print(vote)\n",
    "    answer = np.array([1 if n >= np.round(len(clfs)) else 0 for n in vote])\n",
    "    pred = get_metrics(true_labels=y_test, \n",
    "                predicted_labels=answer)\n",
    "    return pred\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcowang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "      fit_intercept=True, max_iter=None, n_iter=15, n_iter_no_change=5,\n",
      "      n_jobs=None, penalty=None, random_state=42, shuffle=True, tol=0.001,\n",
      "      validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "[2. 2. 2. 0. 1. 0. 2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 0. 0. 2. 2. 2. 1. 1. 2. 0. 0. 0. 2. 1. 2. 0. 0. 2. 0. 0. 1. 2. 2.\n",
      " 2. 0. 2. 2. 0. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 0. 2. 0. 2. 2. 0. 1. 2. 0. 2. 0. 0. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 2. 0.\n",
      " 0. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 2. 0. 2. 2. 0. 0. 0. 0. 2.\n",
      " 1. 2. 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 1. 0. 0.\n",
      " 2. 0. 0. 2. 0. 1. 2. 2. 0. 0. 0. 2. 0. 0. 2. 0. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 2. 2. 0. 0. 0. 2. 0. 0. 2. 0. 0. 2. 0. 2. 0. 0. 0. 2. 2. 0. 0. 2. 2. 0.\n",
      " 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 2. 0. 0.\n",
      " 2. 1. 0. 0. 2. 0. 2. 0. 2. 0. 2. 2. 2. 0. 0. 2. 2. 1. 2. 0. 2. 0. 2. 0.\n",
      " 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 2. 0. 2. 1.\n",
      " 1. 0. 2. 0. 0. 2. 2. 2. 0. 2. 2. 0. 0. 2. 2. 0. 2. 2. 0. 0. 0. 2. 2. 1.\n",
      " 2. 2. 2. 2. 0. 2. 0. 2. 2. 0. 0. 0. 1. 2. 0. 0. 2. 0. 0. 2. 0. 0. 0. 2.\n",
      " 2. 2. 1. 1. 0. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 1. 0. 2. 2. 0. 0. 0. 2. 0.\n",
      " 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 0. 1. 2. 2. 0. 0. 0. 2.\n",
      " 1. 0. 0. 2. 0. 0. 0. 2. 0. 0. 2. 0. 2. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 2. 1. 2. 1. 0. 2. 2. 0. 2. 2. 0. 0. 2. 1. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 2. 0. 0. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2. 2. 0. 0. 0. 1.\n",
      " 0. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 2. 0. 2.\n",
      " 0. 2. 2. 2. 2. 0. 2. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 0. 2. 2. 0. 0. 0. 0.\n",
      " 2. 0. 2. 0. 0. 2. 2. 0. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 2. 0. 0. 2. 2. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 0.\n",
      " 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 2.]\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[3. 3. 3. 0. 1. 0. 3. 3. 3. 0. 0. 0. 0. 3. 3. 3. 0. 0. 3. 3. 3. 1. 3. 0.\n",
      " 3. 3. 0. 0. 3. 3. 3. 2. 2. 3. 0. 0. 1. 3. 2. 3. 0. 0. 2. 0. 0. 2. 3. 3.\n",
      " 3. 0. 3. 3. 0. 0. 3. 3. 3. 3. 0. 3. 2. 3. 3. 0. 3. 3. 3. 0. 3. 3. 2. 3.\n",
      " 0. 3. 0. 3. 3. 1. 1. 3. 0. 3. 0. 0. 3. 0. 3. 0. 3. 0. 0. 3. 3. 0. 3. 0.\n",
      " 1. 3. 0. 3. 3. 3. 3. 3. 0. 0. 0. 3. 0. 3. 0. 3. 0. 3. 2. 0. 0. 0. 0. 3.\n",
      " 1. 3. 0. 0. 3. 1. 3. 2. 0. 3. 3. 0. 1. 0. 3. 0. 3. 3. 3. 0. 0. 1. 0. 0.\n",
      " 3. 0. 0. 3. 0. 2. 2. 3. 0. 0. 0. 3. 0. 0. 3. 0. 3. 0. 3. 0. 3. 3. 3. 3.\n",
      " 3. 3. 0. 0. 1. 3. 0. 0. 3. 0. 0. 3. 0. 3. 0. 0. 1. 3. 2. 0. 0. 3. 3. 0.\n",
      " 1. 0. 3. 3. 0. 3. 3. 2. 3. 3. 3. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 3. 0. 0.\n",
      " 3. 1. 0. 0. 3. 0. 3. 0. 3. 0. 3. 3. 3. 0. 0. 3. 3. 1. 3. 0. 3. 0. 3. 0.\n",
      " 0. 2. 3. 0. 0. 0. 3. 1. 0. 0. 3. 0. 0. 3. 0. 0. 1. 0. 0. 0. 3. 0. 3. 2.\n",
      " 1. 0. 3. 0. 0. 3. 3. 3. 0. 3. 3. 0. 0. 3. 3. 0. 3. 3. 0. 0. 1. 3. 3. 2.\n",
      " 3. 3. 3. 3. 0. 3. 0. 3. 2. 0. 0. 0. 2. 3. 0. 0. 3. 0. 0. 3. 0. 1. 0. 3.\n",
      " 3. 3. 1. 1. 0. 3. 3. 3. 0. 0. 0. 0. 3. 0. 0. 2. 0. 3. 3. 0. 0. 1. 3. 1.\n",
      " 0. 0. 2. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 3. 3. 3. 1. 2. 3. 3. 0. 0. 0. 3.\n",
      " 1. 0. 0. 3. 0. 0. 0. 3. 1. 0. 3. 0. 3. 0. 0. 3. 0. 3. 0. 1. 0. 0. 0. 1.\n",
      " 3. 1. 3. 1. 0. 3. 3. 0. 3. 3. 0. 0. 3. 2. 0. 3. 1. 3. 0. 0. 0. 0. 0. 0.\n",
      " 3. 0. 0. 0. 2. 3. 3. 3. 0. 3. 0. 3. 3. 3. 3. 3. 0. 0. 3. 3. 0. 0. 0. 1.\n",
      " 0. 0. 3. 3. 0. 3. 3. 3. 3. 0. 0. 0. 0. 3. 3. 3. 0. 3. 0. 3. 0. 3. 0. 3.\n",
      " 0. 3. 3. 3. 3. 0. 3. 0. 0. 0. 3. 3. 0. 3. 0. 3. 3. 0. 3. 3. 0. 0. 0. 0.\n",
      " 3. 0. 3. 0. 0. 3. 3. 0. 2. 3. 2. 3. 3. 3. 0. 0. 0. 0. 0. 0. 2. 2. 3. 0.\n",
      " 0. 0. 3. 0. 1. 0. 0. 0. 3. 0. 0. 3. 3. 0. 0. 3. 3. 0. 3. 0. 3. 3. 3. 0.\n",
      " 0. 0. 3. 0. 0. 1. 3. 3. 3. 0. 3. 0. 2. 0. 3. 0. 0. 0. 0. 3. 3. 0. 0. 0.\n",
      " 0. 0. 0. 3. 3. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 3. 0. 3.]\n",
      "[LibSVM]SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=1, gamma=0.01, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=3)\n",
      "[4. 4. 4. 0. 2. 0. 4. 4. 4. 0. 0. 0. 0. 4. 4. 3. 0. 0. 4. 4. 4. 1. 4. 0.\n",
      " 4. 4. 0. 0. 4. 4. 4. 2. 2. 4. 0. 0. 1. 4. 2. 4. 0. 0. 2. 0. 0. 2. 4. 4.\n",
      " 4. 0. 4. 4. 0. 0. 4. 4. 4. 4. 0. 4. 3. 4. 4. 0. 4. 4. 4. 0. 4. 4. 3. 4.\n",
      " 0. 4. 1. 4. 4. 1. 1. 4. 0. 4. 0. 0. 4. 0. 4. 0. 4. 0. 0. 4. 4. 0. 4. 0.\n",
      " 1. 4. 0. 4. 4. 4. 3. 4. 0. 0. 0. 4. 0. 4. 0. 4. 0. 4. 2. 0. 0. 0. 0. 4.\n",
      " 1. 4. 1. 0. 4. 1. 3. 2. 0. 4. 4. 0. 2. 1. 4. 0. 4. 4. 4. 0. 0. 1. 0. 0.\n",
      " 4. 0. 0. 4. 0. 2. 3. 4. 0. 0. 0. 4. 0. 0. 4. 0. 4. 0. 4. 0. 4. 4. 4. 3.\n",
      " 4. 4. 0. 0. 1. 4. 0. 0. 4. 0. 0. 4. 0. 3. 0. 0. 1. 3. 3. 0. 0. 4. 4. 0.\n",
      " 1. 0. 4. 4. 0. 4. 3. 3. 4. 4. 4. 0. 0. 0. 1. 4. 1. 0. 0. 0. 0. 4. 0. 0.\n",
      " 4. 1. 0. 0. 4. 0. 4. 0. 4. 0. 4. 3. 4. 0. 0. 4. 4. 1. 4. 0. 4. 0. 4. 0.\n",
      " 0. 2. 4. 0. 0. 0. 4. 1. 0. 0. 4. 0. 0. 4. 0. 0. 1. 0. 1. 0. 4. 0. 3. 2.\n",
      " 1. 0. 3. 0. 0. 4. 3. 4. 0. 4. 4. 0. 0. 4. 4. 0. 4. 4. 0. 0. 2. 4. 4. 3.\n",
      " 4. 4. 4. 3. 0. 4. 0. 4. 2. 0. 0. 0. 3. 4. 0. 0. 4. 0. 0. 4. 0. 1. 0. 4.\n",
      " 4. 4. 1. 1. 0. 4. 4. 4. 0. 0. 0. 0. 4. 0. 0. 3. 0. 4. 4. 0. 0. 2. 4. 1.\n",
      " 0. 0. 2. 4. 4. 4. 4. 3. 4. 4. 0. 0. 0. 4. 4. 4. 1. 2. 4. 4. 0. 0. 0. 4.\n",
      " 1. 0. 0. 4. 0. 0. 0. 4. 1. 0. 4. 0. 4. 0. 0. 4. 0. 4. 0. 1. 0. 0. 0. 1.\n",
      " 4. 1. 4. 1. 0. 4. 4. 0. 4. 4. 0. 0. 4. 2. 0. 4. 1. 4. 0. 0. 0. 0. 0. 0.\n",
      " 4. 0. 0. 0. 3. 3. 4. 4. 0. 4. 0. 4. 4. 3. 3. 4. 0. 0. 4. 4. 0. 0. 0. 1.\n",
      " 0. 0. 4. 4. 0. 4. 4. 4. 4. 0. 0. 0. 0. 4. 4. 4. 0. 4. 0. 4. 0. 4. 0. 4.\n",
      " 0. 4. 4. 4. 4. 0. 3. 0. 0. 0. 4. 3. 0. 4. 0. 4. 4. 0. 4. 4. 0. 0. 0. 0.\n",
      " 4. 0. 4. 0. 0. 4. 4. 0. 2. 4. 3. 4. 4. 4. 0. 0. 0. 0. 0. 0. 2. 2. 4. 0.\n",
      " 0. 0. 3. 0. 1. 0. 0. 0. 4. 0. 0. 4. 4. 0. 0. 4. 4. 0. 4. 0. 4. 4. 4. 0.\n",
      " 0. 0. 4. 0. 0. 2. 4. 3. 4. 0. 4. 0. 2. 0. 4. 0. 0. 0. 0. 4. 4. 0. 0. 1.\n",
      " 0. 0. 0. 4. 3. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 4. 0. 4.]\n",
      "LogisticRegression(C=0.4393970560760795, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "[5. 5. 5. 0. 3. 0. 5. 5. 5. 0. 0. 0. 0. 5. 5. 3. 0. 0. 5. 5. 5. 1. 5. 0.\n",
      " 5. 5. 0. 0. 5. 5. 5. 2. 2. 5. 0. 0. 1. 5. 3. 5. 0. 0. 2. 0. 0. 2. 5. 5.\n",
      " 5. 0. 5. 5. 0. 0. 5. 5. 5. 5. 0. 5. 4. 5. 5. 0. 5. 5. 5. 0. 5. 5. 4. 5.\n",
      " 0. 5. 1. 5. 5. 1. 1. 5. 0. 5. 0. 0. 5. 0. 5. 0. 5. 0. 0. 5. 5. 0. 5. 0.\n",
      " 1. 5. 0. 5. 5. 5. 3. 5. 0. 0. 0. 5. 0. 5. 0. 5. 0. 5. 2. 0. 0. 0. 0. 5.\n",
      " 1. 5. 1. 0. 5. 1. 4. 2. 0. 5. 5. 0. 3. 1. 5. 0. 5. 5. 5. 0. 0. 1. 0. 0.\n",
      " 5. 0. 0. 5. 0. 3. 4. 5. 0. 0. 0. 5. 0. 0. 5. 0. 5. 0. 5. 0. 5. 5. 5. 3.\n",
      " 5. 5. 0. 0. 2. 5. 0. 0. 5. 0. 0. 5. 0. 3. 0. 1. 1. 4. 4. 0. 0. 5. 5. 0.\n",
      " 1. 0. 5. 5. 0. 5. 4. 4. 5. 5. 5. 0. 0. 0. 1. 5. 1. 0. 0. 0. 0. 5. 0. 0.\n",
      " 4. 1. 0. 0. 5. 0. 5. 0. 5. 0. 5. 3. 5. 0. 0. 5. 5. 1. 5. 0. 5. 0. 5. 0.\n",
      " 0. 2. 5. 0. 0. 0. 5. 1. 0. 0. 5. 0. 0. 5. 0. 0. 1. 0. 1. 0. 5. 0. 3. 2.\n",
      " 2. 0. 3. 0. 0. 5. 3. 5. 0. 5. 5. 0. 0. 5. 5. 0. 5. 5. 0. 0. 2. 5. 5. 4.\n",
      " 5. 5. 5. 3. 0. 5. 0. 5. 2. 0. 0. 0. 3. 5. 0. 0. 5. 0. 0. 5. 0. 1. 0. 5.\n",
      " 5. 5. 1. 2. 0. 5. 5. 5. 0. 0. 0. 0. 5. 0. 0. 4. 0. 5. 5. 0. 0. 2. 5. 1.\n",
      " 0. 0. 3. 5. 5. 5. 5. 3. 5. 5. 0. 0. 0. 5. 5. 5. 1. 2. 5. 5. 0. 0. 0. 5.\n",
      " 1. 0. 0. 5. 0. 0. 0. 5. 1. 0. 5. 0. 5. 0. 0. 5. 0. 5. 0. 1. 0. 0. 0. 1.\n",
      " 5. 1. 5. 1. 0. 5. 5. 0. 5. 5. 0. 0. 5. 2. 0. 5. 1. 5. 0. 0. 0. 0. 0. 0.\n",
      " 5. 0. 0. 0. 4. 3. 5. 5. 0. 5. 0. 5. 5. 3. 4. 5. 0. 0. 5. 5. 0. 0. 0. 1.\n",
      " 0. 0. 5. 5. 0. 5. 5. 5. 5. 0. 0. 0. 0. 5. 5. 5. 0. 5. 0. 5. 0. 5. 0. 5.\n",
      " 0. 5. 5. 5. 5. 0. 4. 0. 0. 0. 5. 3. 0. 5. 0. 5. 5. 0. 5. 5. 0. 0. 0. 0.\n",
      " 5. 0. 5. 0. 0. 5. 5. 0. 2. 5. 4. 5. 5. 5. 0. 0. 0. 0. 0. 0. 2. 2. 5. 0.\n",
      " 0. 0. 4. 0. 1. 0. 0. 0. 5. 0. 0. 5. 5. 0. 0. 5. 5. 0. 5. 0. 5. 5. 5. 0.\n",
      " 0. 0. 5. 0. 0. 2. 5. 4. 5. 0. 5. 0. 3. 0. 5. 0. 0. 0. 0. 5. 5. 0. 0. 1.\n",
      " 0. 0. 0. 5. 4. 0. 0. 0. 0. 1. 0. 0. 0. 0. 2. 1. 0. 5. 0. 5.]\n",
      "Accuracy: 0.9\n",
      "Precision: 1.0\n",
      "Recall: 0.79\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "Majority_vote(bow,clfs=clf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
